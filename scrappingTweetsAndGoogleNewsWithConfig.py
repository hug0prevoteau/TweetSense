# -*- coding: utf-8 -*-
"""nlp_sentiment_analysisAvecGoogleNews.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J3buttidEEZ5Shm7iaSNnsuKeN19ZtmG

## Imports
"""

!pip3 install pymongo

!apt install python-dnspython

!pip3 uninstall dnspython

!pip3 install dnspython

from __future__ import print_function
import tweepy
import json
from pymongo import MongoClient

"""## Connecting to MongoDB

encoding the configs
"""

!pip install cryptography

from cryptography.fernet import Fernet
key = Fernet.generate_key()
print(key)

from cryptography.fernet import Fernet
def cipherText(key , text)
  cipher_suite = Fernet(key)
  ciphered_text = cipher_suite.encrypt(bytes(text,'utf-8'))   #required to be bytes
  return ciphered_text

"""decoding the configs"""

from cryptography.fernet import Fernet
def uncipherText(key , text):
  cipher_suite = Fernet(key)
  unciphered_text = (cipher_suite.decrypt(bytes(text,'utf-8')))
  return unciphered_text

import json

file = open('config.json',) 
config = json.load(file)

MONGO_HOST = uncipherText(config["key"], config["mongoHost"]).decode('utf-8')
CONSUMER_KEY= config["consumerKey"]
CONSUMER_SECRET = uncipherText(config["key"], config["consumerSecret"]).decode('utf-8')
ACCESS_TOKEN = config["accessToken"]
ACCESS_TOKEN_SECRET = uncipherText(config["key"], config["accessTokenSecret"]).decode('utf-8')

WORDS = ['#Trump', '#Biden', 'Trump', 'Biden']

"""## Scrapping tweets"""

class StreamListener(tweepy.StreamListener):
    #This is a class provided by tweepy to access the Twitter Streaming API. 

    def on_connect(self):
        # Called initially to connect to the Streaming API
        print("You are now connected to the streaming API.")
 
    def on_error(self, status_code):
        # On error - if an error occurs, display the error / status code
        print('An Error has occured: ' + repr(status_code))
        return False
 
    def on_data(self, data):
        #This is the meat of the script...it connects to your mongoDB and stores the tweet
        try:
            client = MongoClient(MONGO_HOST)
            
            # Use twitterdb database. If it doesn't exist, it will be created.
            db = client.twitterdb
    
            # Decode the JSON from Twitter
            datajson = json.loads(data)
            
            #grab the 'created_at' data from the Tweet to use for display
            created_at = datajson['created_at']

            #print out a message to the screen that we have collected a tweet
            print("Tweet collected at " + str(created_at))
            
            #insert the data into the mongoDB into a collection called twitter_search
            #if twitter_search doesn't exist, it will be created.
            db.twitter_search.insert(datajson)
        except Exception as e:
            print(e)

auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)
auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)
#Set up the listener. The 'wait_on_rate_limit=True' is needed to help with Twitter API rate limiting.
listener = StreamListener(api=tweepy.API(wait_on_rate_limit=True )) 
streamer = tweepy.Stream(auth=auth, listener=listener)
print("Tracking: " + str(WORDS))
streamer.filter(track=WORDS)

"""## Scrapping Google News"""

!pip install GoogleNews

import json
import GoogleNews
from GoogleNews import GoogleNews

"""Execution de la requette de scrapping des new en anglais """

googlenews = GoogleNews(lang='en', period='7d')
query = "climate change"
googlenews.get_news(query)

"""création de la fonciton d'ajout des données dans la mongoDB"""

def addingNewsInDB(googleNewResult , googleNewsText):
  try:
    #connecting to the mongoDB
    client = MongoClient(MONGO_HOST)
            
    # Use twitterdb database. If it doesn't exist, it will be created.
    db = client.twitterdb
    dataFinal=[]
    for i in range(len(googleNewResult)):
      data= googleNewResult[i]
      data['text']=googleNewsText[i]
      dateString=data['datetime'].strftime("%a %B %d %H:%M:%S +0000 %Y")
      data['created_at']=dateString
      json_data=json.dumps(data, indent=4, sort_keys=True, default=str)
      print("News collected at " + dateString)     
      #insert the data into the mongoDB into a collection called google_search
      #if google_search doesn't exist, it will be created.
      db.google_search.insert(json.loads(json_data))
  except Exception as e:
    print(e)

"""Execution de l'ajout des news dans la collection google_search de la mongoDB"""

addingNewsInDB(googlenews.results() , googlenews.get_texts())

"""## Query MongoDB"""

mydb = client["twitterdb"]
mycol = mydb["twitter_search"]
token = 'AI'

myquery = { 'text': {'$regex' : f'.*{token}.*'} }

mydoc = mycol.find(myquery)

"""## Sentiment analysis"""

import tensorflow_hub as hub
import pandas as pd
import re
import string

df = pd.DataFrame(list(mydoc))

def remove_punct(text):
    text  = "".join([char for char in text if char not in string.punctuation])
    text = re.sub('[0-9]+', '', text)
    return text

df['text_cleaned'] = df['text'].apply(lambda x: remove_punct(x))

df  = pd.DataFrame(df[['text_cleaned']])

df